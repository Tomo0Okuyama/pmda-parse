#!/usr/bin/env python3
"""
PMDAåŒ»è–¬å“ãƒ‡ãƒ¼ã‚¿JSONç”Ÿæˆãƒ„ãƒ¼ãƒ«ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰

ãƒãƒƒãƒåˆ†å‰² + ãƒ¯ãƒ¼ã‚«ãƒ¼ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã‚’å®Ÿç¾ï¼š
- ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒãƒƒãƒåˆ†å‰²å‡¦ç†
- å‹•çš„è² è·åˆ†æ•£ã«ã‚ˆã‚‹æœ€é©ãƒ¯ãƒ¼ã‚«ãƒ¼é…ç½®
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸãƒãƒƒãƒã‚µã‚¤ã‚ºè‡ªå‹•èª¿æ•´
- ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡æœ€é©åŒ–ã«ã‚ˆã‚‹ä¸¦åˆ—å‡¦ç†åŠ¹ç‡å‘ä¸Š
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†çµ±è¨ˆã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
"""

import os
import sys
import json
import time
import argparse
import glob
import re
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import threading
import psutil
import gc

# ãƒ‘ã‚¹ã‚’èª¿æ•´ã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ¼ã‚µãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from parsers.shared_xml_processor import SharedXMLProcessor
from utils.file_processor import find_duplicate_files

class BatchProcessor:
    """
    ãƒãƒƒãƒåˆ†å‰²å‡¦ç†ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, files: List[str], batch_size: Optional[int] = None, memory_limit_mb: int = 1024):
        """
        åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰
        
        Args:
            files (List[str]): å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ
            batch_size (int): ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆè‡ªå‹•è¨ˆç®—ã•ã‚Œã‚‹å ´åˆã¯Noneï¼‰
            memory_limit_mb (int): ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆMBï¼‰
        """
        self.files = files
        self.memory_limit_mb = memory_limit_mb
        self.batch_size = batch_size or self._calculate_optimal_batch_size()
        self.batches = self._create_batches()
        
    def _calculate_optimal_batch_size(self) -> int:
        """
        ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒã«åŸºã¥ã„ã¦æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        
        Returns:
            int: æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚º
        """
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªæƒ…å ±ã‚’å–å¾—
        memory = psutil.virtual_memory()
        available_memory_mb = memory.available / (1024 * 1024)
        
        # CPUæ•°ã‚’è€ƒæ…®
        cpu_cores = cpu_count()
        
        # å¹³å‡XMLãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ¨å®šï¼ˆç´„100KBï¼‰
        estimated_file_size_mb = 0.1
        
        # ãƒ¡ãƒ¢ãƒªåˆ¶é™å†…ã§ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨ˆç®—
        memory_based_batch_size = int(self.memory_limit_mb / estimated_file_size_mb)
        
        # CPUåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸãƒãƒƒãƒã‚µã‚¤ã‚º
        cpu_based_batch_size = max(10, len(self.files) // (cpu_cores * 4))
        
        # æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ±ºå®šï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ã‚’å«ã‚€ï¼‰
        optimal_batch_size = min(memory_based_batch_size, cpu_based_batch_size, 500)
        
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚ºè‡ªå‹•è¨ˆç®—: {optimal_batch_size}")
        print(f"   - åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {available_memory_mb:.1f}MB")
        print(f"   - CPUæ•°: {cpu_cores}")
        print(f"   - ãƒ¡ãƒ¢ãƒªåˆ¶é™: {self.memory_limit_mb}MB")
        
        return max(optimal_batch_size, 10)  # æœ€ä½10ãƒ•ã‚¡ã‚¤ãƒ«/ãƒãƒƒãƒ
    
    def _create_batches(self) -> List[List[str]]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒã«åˆ†å‰²
        
        Returns:
            List[List[str]]: ãƒãƒƒãƒåˆ†å‰²ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        """
        batches = []
        for i in range(0, len(self.files), self.batch_size):
            batch = self.files[i:i + self.batch_size]
            batches.append(batch)
        
        print(f"   ç·ãƒãƒƒãƒæ•°: {len(batches)}")
        print(f"   å¹³å‡ãƒãƒƒãƒã‚µã‚¤ã‚º: {len(self.files) / len(batches):.1f}ãƒ•ã‚¡ã‚¤ãƒ«/ãƒãƒƒãƒ")
        
        return batches
    
    def get_batches(self) -> List[List[str]]:
        """
        ãƒãƒƒãƒãƒªã‚¹ãƒˆã‚’å–å¾—
        
        Returns:
            List[List[str]]: ãƒãƒƒãƒåˆ†å‰²ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        """
        return self.batches

def process_batch_worker(batch_files: List[str], batch_id: int) -> Tuple[int, List[Dict[str, Any]], Dict[str, int]]:
    """
    ãƒãƒƒãƒå‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ï¼ˆãƒ—ãƒ­ã‚»ã‚¹é–“ã§å®Ÿè¡Œï¼‰
    
    Args:
        batch_files (List[str]): ãƒãƒƒãƒå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        batch_id (int): ãƒãƒƒãƒID
        
    Returns:
        Tuple[int, List[Dict[str, Any]], Dict[str, int]]: (ãƒãƒƒãƒID, åŒ»è–¬å“ãƒ‡ãƒ¼ã‚¿, çµ±è¨ˆæƒ…å ±)
    """
    batch_medicines = []
    batch_stats = {
        'processed_files': 0,
        'error_files': 0,
        'medicines_count': 0,
        'xml_parse_count': 0
    }
    
    try:
        for file_path in batch_files:
            try:
                # SharedXMLProcessorã§è¶…é«˜é€Ÿå‡¦ç†
                processor = SharedXMLProcessor(file_path)
                medicines_data = processor.process_all_brands()
                
                if medicines_data:
                    batch_medicines.extend(medicines_data)
                    batch_stats['medicines_count'] += len(medicines_data)
                    batch_stats['processed_files'] += 1
                    batch_stats['xml_parse_count'] += 1
                else:
                    batch_stats['error_files'] += 1
                    
            except Exception as e:
                batch_stats['error_files'] += 1
                print(f"   ãƒãƒƒãƒ{batch_id}: ã‚¨ãƒ©ãƒ¼ {os.path.basename(file_path)}: {e}")
                continue
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        gc.collect()
        
        return batch_id, batch_medicines, batch_stats
        
    except Exception as e:
        print(f"   ãƒãƒƒãƒ{batch_id}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return batch_id, [], batch_stats

class PMDAJSONGeneratorOptimized:
    """
    æœ€é©åŒ–ç‰ˆJSONã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼
    
    ã€æœ€é©åŒ–ãƒã‚¤ãƒ³ãƒˆã€‘
    1. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒãƒƒãƒåˆ†å‰²å‡¦ç†
    2. ãƒ—ãƒ­ã‚»ã‚¹/ã‚¹ãƒ¬ãƒƒãƒ‰ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ä¸¦åˆ—å‡¦ç†
    3. å‹•çš„è² è·åˆ†æ•£ã¨ãƒ¡ãƒ¢ãƒªç®¡ç†
    4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†çµ±è¨ˆã¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
    5. ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æœ€é©æ´»ç”¨
    """
    
    def __init__(self, input_directory: str, output_file: str, 
                 max_workers: Optional[int] = None, 
                 batch_size: Optional[int] = None,
                 memory_limit_mb: int = 2048,
                 use_process_pool: bool = True):
        """
        åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰
        
        Args:
            input_directory (str): PMDAãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            output_file (str): å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            max_workers (Optional[int]): ä¸¦åˆ—å‡¦ç†ã®æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
            batch_size (Optional[int]): ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆè‡ªå‹•è¨ˆç®—ã®å ´åˆã¯Noneï¼‰
            memory_limit_mb (int): ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆMBï¼‰
            use_process_pool (bool): ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
        """
        self.input_directory = input_directory
        self.output_file = output_file
        self.max_workers = max_workers or min(cpu_count(), 16)  # CPUæ€§èƒ½ã«å¿œã˜ã¦ä¸Šé™æ‹¡å¤§
        self.batch_size = batch_size
        self.memory_limit_mb = memory_limit_mb
        self.use_process_pool = use_process_pool
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®å–å¾—
        self.system_info = {
            'cpu_count': cpu_count(),
            'total_memory_gb': psutil.virtual_memory().total / (1024**3),
            'available_memory_gb': psutil.virtual_memory().available / (1024**3)
        }
        
        self.statistics = {
            'total_files_found': 0,
            'duplicate_files': 0,
            'processed_files': 0,
            'error_files': 0,
            'medicines_count': 0,
            'vectors_count': defaultdict(int),
            'medicines_with_clinical_info': defaultdict(int),
            'processing_time': 0,
            'files_per_second': 0,
            'medicines_per_second': 0,
            'parallel_workers': 0,
            'xml_parse_count': 0,
            'batch_count': 0,
            'average_batch_time': 0,
            'memory_efficiency_ratio': 0,
            'batch_processing_times': []
        }
        
        self.lock = threading.Lock()
        
    def _update_statistics_batch(self, batch_stats: Dict[str, int], batch_medicines: List[Dict[str, Any]]):
        """
        ãƒãƒƒãƒå‡¦ç†çµæœã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°
        
        Args:
            batch_stats (Dict[str, int]): ãƒãƒƒãƒçµ±è¨ˆæƒ…å ±
            batch_medicines (List[Dict[str, Any]]): ãƒãƒƒãƒã§å‡¦ç†ã•ã‚ŒãŸåŒ»è–¬å“ãƒ‡ãƒ¼ã‚¿
        """
        with self.lock:
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çµ±è¨ˆ
            self.statistics['processed_files'] += batch_stats['processed_files']
            self.statistics['error_files'] += batch_stats['error_files']
            self.statistics['medicines_count'] += batch_stats['medicines_count']
            self.statistics['xml_parse_count'] += batch_stats['xml_parse_count']
            
            # å„åŒ»è–¬å“ã®è‡¨åºŠæƒ…å ±ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            for medicine in batch_medicines:
                clinical_info = medicine.get('clinical_info', {})
                
                # åŠ¹èƒ½ãƒ»åŠ¹æœ
                if clinical_info.get('indications'):
                    self.statistics['medicines_with_clinical_info']['indications'] += 1
                    self.statistics['vectors_count']['indications'] += len(clinical_info['indications'])
                
                # ç”¨æ³•ãƒ»ç”¨é‡
                if clinical_info.get('dosage'):
                    self.statistics['medicines_with_clinical_info']['dosage'] += 1
                    self.statistics['vectors_count']['dosage'] += len(clinical_info['dosage'])
                
                # ç¦å¿Œ
                if clinical_info.get('contraindications'):
                    self.statistics['medicines_with_clinical_info']['contraindications'] += 1
                    self.statistics['vectors_count']['contraindications'] += len(clinical_info['contraindications'])
                
                # è­¦å‘Šãƒ»æ³¨æ„äº‹é …
                if clinical_info.get('warnings'):
                    self.statistics['medicines_with_clinical_info']['warnings'] += 1
                    self.statistics['vectors_count']['warnings'] += len(clinical_info['warnings'])
                
                # å‰¯ä½œç”¨
                if clinical_info.get('side_effects'):
                    self.statistics['medicines_with_clinical_info']['side_effects'] += 1
                    self.statistics['vectors_count']['side_effects'] += len(clinical_info['side_effects'])
                
                # ç›¸äº’ä½œç”¨
                if clinical_info.get('interactions'):
                    self.statistics['medicines_with_clinical_info']['interactions'] += 1
                    self.statistics['vectors_count']['interactions'] += len(clinical_info['interactions'])
                
                # æˆåˆ†ãƒ»å«é‡
                if clinical_info.get('compositions'):
                    self.statistics['medicines_with_clinical_info']['compositions'] += 1
                    self.statistics['vectors_count']['compositions'] += len(clinical_info['compositions'])
                
                # æœ‰åŠ¹æˆåˆ†
                if clinical_info.get('active_ingredients'):
                    self.statistics['medicines_with_clinical_info']['active_ingredients'] += 1
                    self.statistics['vectors_count']['active_ingredients'] += len(clinical_info['active_ingredients'])
    
    def generate_json_optimized(self):
        """
        æœ€é©åŒ–PMDAãƒ‡ãƒ¼ã‚¿ã‹ã‚‰JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        """
        start_time = time.time()
        
        print("=== PMDAåŒ»è–¬å“ãƒ‡ãƒ¼ã‚¿JSONç”Ÿæˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰ ===")
        print(f"ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.max_workers}")
        print(f"ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ä½¿ç”¨: {'ã¯ã„' if self.use_process_pool else 'ã„ã„ãˆï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ï¼‰'}")
        print(f"ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        print(f"  - CPUæ•°: {self.system_info['cpu_count']}")
        print(f"  - ç·ãƒ¡ãƒ¢ãƒª: {self.system_info['total_memory_gb']:.1f}GB")
        print(f"  - åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {self.system_info['available_memory_gb']:.1f}GB")
        print(f"  - ãƒ¡ãƒ¢ãƒªåˆ¶é™: {self.memory_limit_mb}MB")
        
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã¨ã‚µã‚¤ã‚ºç¢ºèª
        print("\n1. ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ä¸­...")
        xml_sgml_path = os.path.join(self.input_directory, 'SGML_XML')
        
        if not os.path.exists(xml_sgml_path):
            raise ValueError(f"SGML_XMLãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {xml_sgml_path}")
        
        # å…¨ã¦ã®XML/SGMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        all_files = []
        for root, _, files in os.walk(xml_sgml_path):
            for file in files:
                if file.endswith(('.xml', '.sgml')):
                    all_files.append(os.path.join(root, file))
        
        self.statistics['total_files_found'] = len(all_files)
        print(f"   ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.statistics['total_files_found']}")
        
        # 2. é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®é™¤å»ï¼ˆSHA-256ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ï¼‰
        print("2. é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«é™¤å»ä¸­...")
        duplicates = find_duplicate_files(xml_sgml_path, ['.xml', '.sgml'])
        
        # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å»
        unique_files = []
        duplicate_hashes = set()
        
        for file_path in all_files:
            file_hash = None
            for hash_val, file_list in duplicates.items():
                if file_path in file_list:
                    file_hash = hash_val
                    break
            
            if file_hash and file_hash in duplicate_hashes:
                self.statistics['duplicate_files'] += 1
                continue
            elif file_hash:
                duplicate_hashes.add(file_hash)
            
            unique_files.append(file_path)
        
        print(f"   é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.statistics['duplicate_files']}")
        print(f"   å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(unique_files)}")
        
        # 3. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒãƒƒãƒåˆ†å‰²
        print("3. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒãƒƒãƒåˆ†å‰²ä¸­...")
        batch_processor = BatchProcessor(
            files=unique_files, 
            batch_size=self.batch_size,
            memory_limit_mb=self.memory_limit_mb
        )
        batches = batch_processor.get_batches()
        self.statistics['batch_count'] = len(batches)
        
        # 4. æœ€é©åŒ–ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹åŒ»è–¬å“ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        print("4. æœ€é©åŒ–ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†é–‹å§‹...")
        all_medicines = []
        
        # ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ã¾ãŸã¯ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’é¸æŠ
        executor_class = ProcessPoolExecutor if self.use_process_pool else ThreadPoolExecutor
        
        with executor_class(max_workers=self.max_workers) as executor:
            # ãƒãƒƒãƒå‡¦ç†ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
            future_to_batch = {
                executor.submit(process_batch_worker, batch, batch_id): (batch_id, len(batch))
                for batch_id, batch in enumerate(batches)
            }
            
            completed_batches = 0
            batch_start_time = time.time()
            
            # å®Œäº†ã—ãŸãƒãƒƒãƒã‹ã‚‰é †æ¬¡çµæœã‚’å–å¾—
            for future in as_completed(future_to_batch):
                batch_id, batch_size = future_to_batch[future]
                completed_batches += 1
                
                try:
                    result_batch_id, batch_medicines, batch_stats = future.result()
                    
                    if batch_medicines:
                        all_medicines.extend(batch_medicines)
                        self._update_statistics_batch(batch_stats, batch_medicines)
                    
                    # ãƒãƒƒãƒå‡¦ç†æ™‚é–“ã‚’è¨˜éŒ²
                    batch_time = time.time() - batch_start_time
                    self.statistics['batch_processing_times'].append(batch_time)
                    
                    # é€²æ—è¡¨ç¤ºï¼ˆãƒãƒƒãƒã”ã¨ï¼‰
                    progress = (completed_batches / len(batches)) * 100
                    current_speed = completed_batches / (time.time() - start_time) if (time.time() - start_time) > 0 else 0
                    
                    print(f"   ãƒãƒƒãƒé€²æ—: {completed_batches}/{len(batches)} ({progress:.1f}%) "
                          f"| ãƒãƒƒãƒ{result_batch_id}: {batch_stats['medicines_count']}ä»¶ "
                          f"| é€Ÿåº¦: {current_speed:.1f} batches/sec")
                    
                    batch_start_time = time.time()
                    
                except Exception as e:
                    print(f"   ãƒãƒƒãƒ{batch_id}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    with self.lock:
                        self.statistics['error_files'] += batch_size
        
        # 5. JSONå‡ºåŠ›
        print("5. JSONå‡ºåŠ›ä¸­...")
        self._save_json_optimized(all_medicines)
        
        # 6. å‡¦ç†æ™‚é–“ã¨çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
        end_time = time.time()
        self.statistics['processing_time'] = end_time - start_time
        self.statistics['parallel_workers'] = self.max_workers
        
        # ãƒãƒƒãƒå‡¦ç†çµ±è¨ˆã®è¨ˆç®—
        if self.statistics['batch_processing_times']:
            self.statistics['average_batch_time'] = sum(self.statistics['batch_processing_times']) / len(self.statistics['batch_processing_times'])
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ¯”ç‡ã®è¨ˆç®—
        traditional_parse_count = self.statistics['processed_files'] * 9
        actual_parse_count = self.statistics['xml_parse_count']
        self.statistics['memory_efficiency_ratio'] = (1 - (actual_parse_count / traditional_parse_count)) * 100 if traditional_parse_count > 0 else 0
        
        if self.statistics['processing_time'] > 0:
            self.statistics['files_per_second'] = self.statistics['processed_files'] / self.statistics['processing_time']
            self.statistics['medicines_per_second'] = self.statistics['medicines_count'] / self.statistics['processing_time']
        
        # 7. çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self._print_summary_optimized()
    
    def _save_json_optimized(self, medicines: List[Dict[str, Any]]):
        """
        åŒ»è–¬å“ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        
        Args:
            medicines (List[Dict[str, Any]]): ä¿å­˜ã™ã‚‹åŒ»è–¬å“ãƒ‡ãƒ¼ã‚¿
        """
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir = os.path.dirname(self.output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # JSONå‡ºåŠ›ï¼ˆUTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆä»˜ãï¼‰
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(medicines, f, ensure_ascii=False, indent=2)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—ãƒ»è¡¨ç¤º
        file_size = os.path.getsize(self.output_file)
        file_size_mb = file_size / (1024 * 1024)
        
        print(f"   å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {self.output_file}")
        print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.2f} MB")
    
    def _print_summary_optimized(self):
        """
        å‡¦ç†ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤ºï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        """
        print("\n=== å‡¦ç†ã‚µãƒãƒªãƒ¼ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰ ===")
        print(f"ç™ºè¦‹ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.statistics['total_files_found']:,}")
        print(f"é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.statistics['duplicate_files']:,}")
        print(f"å‡¦ç†æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.statistics['processed_files']:,}")
        print(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.statistics['error_files']:,}")
        print(f"ç”Ÿæˆã•ã‚ŒãŸåŒ»è–¬å“ã‚¨ãƒ³ãƒˆãƒªãƒ¼æ•°: {self.statistics['medicines_count']:,}")
        print(f"å‡¦ç†æ™‚é–“: {self.statistics['processing_time']:.2f}ç§’")
        print(f"ä¸¦åˆ—ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {self.statistics['parallel_workers']}")
        
        # ãƒãƒƒãƒå‡¦ç†çµ±è¨ˆ
        print(f"\n=== ãƒãƒƒãƒå‡¦ç†çµ±è¨ˆ ===")
        print(f"ç·ãƒãƒƒãƒæ•°: {self.statistics['batch_count']}")
        print(f"å¹³å‡ãƒãƒƒãƒå‡¦ç†æ™‚é–“: {self.statistics['average_batch_time']:.2f}ç§’")
        if self.statistics['batch_count'] > 0:
            print(f"ãƒãƒƒãƒä¸¦åˆ—åŠ¹ç‡: {self.statistics['batch_count'] / self.statistics['processing_time']:.2f} batches/sec")
        
        # æœ€é©åŒ–åŠ¹æœã®è¡¨ç¤º
        print(f"\n=== æœ€é©åŒ–åŠ¹æœ ===")
        print(f"XMLãƒ‘ãƒ¼ã‚¹å›æ•°: {self.statistics['xml_parse_count']:,}å›")
        traditional_parse_count = self.statistics['processed_files'] * 9
        print(f"å¾“æ¥æ–¹å¼ã®ãƒ‘ãƒ¼ã‚¹å›æ•°: {traditional_parse_count:,}å›ï¼ˆäºˆæƒ³ï¼‰")
        print(f"XMLãƒ‘ãƒ¼ã‚¹å‰Šæ¸›ç‡: {self.statistics['memory_efficiency_ratio']:.1f}%")
        
        # å‡¦ç†é€Ÿåº¦ã®è¡¨ç¤º
        if self.statistics['processing_time'] > 0:
            print(f"\n=== å‡¦ç†é€Ÿåº¦ ===")
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é€Ÿåº¦: {self.statistics['files_per_second']:.2f} files/sec")
            print(f"åŒ»è–¬å“ç”Ÿæˆé€Ÿåº¦: {self.statistics['medicines_per_second']:.2f} medicines/sec")
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡
        print(f"\n=== ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ ===")
        print(f"CPUä½¿ç”¨åŠ¹ç‡: {(self.statistics['parallel_workers'] / self.system_info['cpu_count']) * 100:.1f}%")
        print(f"æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {(self.statistics['batch_count'] * self.memory_limit_mb) / 1024:.1f}GB")
        
        # æŠ½å‡ºã•ã‚ŒãŸè‡¨åºŠæƒ…å ±ã®è©³ç´°
        print("\n=== æŠ½å‡ºã•ã‚ŒãŸè‡¨åºŠæƒ…å ± ===")
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®é †åºã«å¤‰æ›´
        clinical_info_order = [
            ('åŠ¹èƒ½ãƒ»åŠ¹æœ', 'indications'),
            ('ç”¨æ³•ãƒ»ç”¨é‡', 'dosage'),
            ('æˆåˆ†ãƒ»å«é‡', 'compositions'),
            ('æœ‰åŠ¹æˆåˆ†', 'active_ingredients'),
            ('ç¦å¿Œ', 'contraindications'),
            ('å‰¯ä½œç”¨', 'side_effects'),
            ('ç›¸äº’ä½œç”¨', 'interactions'),
            ('è­¦å‘Šãƒ»æ³¨æ„', 'warnings')
        ]
        
        # æ—¥æœ¬èªæ–‡å­—å¹…ã‚’è€ƒæ…®ã—ãŸè¡¨ç¤ºå¹…è¨ˆç®—
        def get_display_width(text):
            """æ—¥æœ¬èªæ–‡å­—ã‚’è€ƒæ…®ã—ãŸè¡¨ç¤ºå¹…ã‚’è¨ˆç®—"""
            width = 0
            for char in text:
                if ord(char) > 127:  # æ—¥æœ¬èªæ–‡å­—
                    width += 2
                else:  # è‹±æ•°å­—
                    width += 1
            return width
        
        # æœ€å¤§å¹…ã‚’è¨ˆç®—
        max_width = max(get_display_width(display_name) for display_name, _ in clinical_info_order)
        max_width = max(max_width, get_display_width('ç·è‡¨åºŠæƒ…å ±æ•°'))
        
        for display_name, key in clinical_info_order:
            count = self.statistics['vectors_count'].get(key, 0)
            current_width = get_display_width(display_name)
            padding = max_width - current_width
            print(f"{display_name}{' ' * padding}: {count:8,}ä»¶")

        total_vectors = sum(self.statistics['vectors_count'].values())
        total_width = get_display_width('ç·è‡¨åºŠæƒ…å ±æ•°')
        total_padding = max_width - total_width
        print(f"ç·è‡¨åºŠæƒ…å ±æ•°{' ' * total_padding}: {total_vectors:8,}ä»¶")

        # åŒ»ç™‚æƒ…å ±ç¨®åˆ¥æ¯ã®åŒ»è–¬å“æ•°
        print("\n=== åŒ»ç™‚æƒ…å ±ç¨®åˆ¥æ¯ã®åŒ»è–¬å“æ•° ===")
        # æŠ½å‡ºã•ã‚ŒãŸè‡¨åºŠæƒ…å ±ã¨åŒã˜é †åºã«å¤‰æ›´
        info_order = [
            ('åŠ¹èƒ½ãƒ»åŠ¹æœ', 'indications'),
            ('ç”¨æ³•ãƒ»ç”¨é‡', 'dosage'),
            ('æˆåˆ†ãƒ»å«é‡', 'compositions'),
            ('æœ‰åŠ¹æˆåˆ†', 'active_ingredients'),
            ('ç¦å¿Œ', 'contraindications'),
            ('å‰¯ä½œç”¨', 'side_effects'),
            ('ç›¸äº’ä½œç”¨', 'interactions'),
            ('è­¦å‘Šãƒ»æ³¨æ„', 'warnings')
        ]
        
        # æœ€å¤§å¹…ã‚’è¨ˆç®—
        max_width_info = max(get_display_width(display_name) for display_name, _ in info_order)
        
        for display_name, key in info_order:
            count = self.statistics['medicines_with_clinical_info'].get(key, 0)
            percentage = (count / self.statistics['medicines_count']) * 100 if self.statistics['medicines_count'] > 0 else 0
            current_width = get_display_width(display_name)
            padding = max_width_info - current_width
            print(f"{display_name}{' ' * padding}: {count:8,}ä»¶ ({percentage:5.1f}%)")
        
        # å…¨ç¨®é¡ã®åŒ»ç™‚æƒ…å ±ã‚’æŒã¤åŒ»è–¬å“æ•°ã‚’è¨ˆç®—
        if self.statistics['medicines_with_clinical_info']:
            min_count = min(self.statistics['medicines_with_clinical_info'].values())
            print(f"\nå…¨ç¨®é¡ã®åŒ»ç™‚æƒ…å ±ã‚’æŒã¤åŒ»è–¬å“: {min_count:,}ä»¶")

def find_pmda_directories() -> List[str]:
    """
    ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§pmda_all_nnnnnnnnå½¢å¼ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
    
    Returns:
        List[str]: è¦‹ã¤ã‹ã£ãŸPMDAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
    """
    pattern = 'pmda_all_[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]'
    pmda_dirs = []
    
    for item in glob.glob(pattern):
        if os.path.isdir(item):
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåãŒæ­£ç¢ºã«ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            dir_name = os.path.basename(item)
            if re.match(r'^pmda_all_\d{8}$', dir_name):
                pmda_dirs.append(item)
    
    return sorted(pmda_dirs)  # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆ

def auto_detect_pmda_directory() -> str:
    """
    PMDAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•æ¤œå‡º
    
    Returns:
        str: æ¤œå‡ºã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
        
    Raises:
        SystemExit: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€è¤‡æ•°è¦‹ã¤ã‹ã£ãŸå ´åˆ
    """
    pmda_dirs = find_pmda_directories()
    
    if len(pmda_dirs) == 0:
        print("ã‚¨ãƒ©ãƒ¼: pmda_all_nnnnnnnnå½¢å¼ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("1. PMDAã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€pmda_all_nnnnnnnnå½¢å¼ã§å±•é–‹ã—ã¦ãã ã•ã„")
        print("   ä¾‹: pmda_all_20250709")
        print("2. ã¾ãŸã¯ --input ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æ˜ç¤ºçš„ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
        print("   ä¾‹: python src/pmda_json_generator_optimized.py --input /path/to/pmda_data")
        sys.exit(1)
    
    if len(pmda_dirs) > 1:
        print(f"ã‚¨ãƒ©ãƒ¼: è¤‡æ•°ã®PMDAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {', '.join(pmda_dirs)}")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("--input ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ä½¿ç”¨ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã—ã¦ãã ã•ã„")
        print("ä¾‹:")
        for pmda_dir in pmda_dirs:
            print(f"  python src/pmda_json_generator_optimized.py --input {pmda_dir}")
        sys.exit(1)
    
    detected_dir = pmda_dirs[0]
    print(f"PMDAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•æ¤œå‡ºã—ã¾ã—ãŸ: {detected_dir}")
    return detected_dir

def main():
    """
    ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    parser = argparse.ArgumentParser(
        description='PMDAåŒ»è–¬å“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨JSONã‚’ç”Ÿæˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®pmda_all_nnnnnnnnã‚’è‡ªå‹•æ¤œå‡º
  python src/pmda_json_generator_optimized.py
  
  # ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
  python src/pmda_json_generator_optimized.py --input pmda_all_20250709
  
  # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã‚’æŒ‡å®š
  python src/pmda_json_generator_optimized.py --output custom.json --workers 8
        """
    )
    parser.add_argument(
        '--input', '-i',
        default=None,
        help='PMDAãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ï¼ˆæœªæŒ‡å®šã®å ´åˆã¯è‡ªå‹•æ¤œå‡ºï¼‰'
    )
    parser.add_argument(
        '--output', '-o',
        default='pmda_medicines_optimized.json',
        help='å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: pmda_medicines_optimized.jsonï¼‰'
    )
    parser.add_argument(
        '--workers', '-w',
        type=int,
        default=None,
        help='ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: CPUæ•°ã¨16ã®å°ã•ã„æ–¹ï¼‰'
    )
    parser.add_argument(
        '--batch-size', '-b',
        type=int,
        default=None,
        help='ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è‡ªå‹•è¨ˆç®—ï¼‰'
    )
    parser.add_argument(
        '--memory-limit', '-m',
        type=int,
        default=2048,
        help='ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆMBã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2048ï¼‰'
    )
    parser.add_argument(
        '--use-threads',
        action='store_true',
        help='ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ã®ä»£ã‚ã‚Šã«ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½¿ç”¨'
    )
    
    args = parser.parse_args()
    
    # å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ±ºå®š
    if args.input is None:
        # è‡ªå‹•æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰
        input_directory = auto_detect_pmda_directory()
    else:
        # æ˜ç¤ºçš„æŒ‡å®šãƒ¢ãƒ¼ãƒ‰
        input_directory = args.input
        if not os.path.exists(input_directory):
            print(f"ã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸå…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_directory}")
            print("\nä½¿ç”¨æ–¹æ³•:")
            print("1. æ­£ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            print("2. ã¾ãŸã¯ --input ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’çœç•¥ã—ã¦è‡ªå‹•æ¤œå‡ºã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
            return 1
        
        # SGML_XMLãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
        sgml_xml_path = os.path.join(input_directory, 'SGML_XML')
        if not os.path.exists(sgml_xml_path):
            print(f"ã‚¨ãƒ©ãƒ¼: SGML_XMLãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sgml_xml_path}")
            print(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒPMDAãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return 1
    
    # JSONç”Ÿæˆå™¨ã‚’åˆæœŸåŒ–ã—ã¦å®Ÿè¡Œ
    generator = PMDAJSONGeneratorOptimized(
        input_directory=input_directory,
        output_file=args.output,
        max_workers=args.workers,
        batch_size=args.batch_size,
        memory_limit_mb=args.memory_limit,
        use_process_pool=not args.use_threads
    )
    
    try:
        generator.generate_json_optimized()
        print(f"\nğŸš€ æœ€é©åŒ–JSONç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ: {args.output}")
        print("âš¡ ãƒãƒƒãƒåˆ†å‰² + ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã‚’å®Ÿç¾")
    except Exception as e:
        print(f"\nã‚¨ãƒ©ãƒ¼: JSONç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1
    
    return 0

if __name__ == '__main__':
    main()